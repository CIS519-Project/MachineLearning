%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%% ICML 2014 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Use the following line _only_ if you're still using LaTeX 2.09.
%\documentstyle[icml2014,epsf,natbib]{article}
% If you rely on Latex2e packages, like most moden people use this:
\documentclass{article}

% use Times
\usepackage{times}
% For figures
\usepackage{graphicx} % more modern
%\usepackage{epsfig} % less modern
%\usepackage{subfigure}

% For citations
\usepackage{natbib}

% For algorithms
\usepackage{algorithm}
\usepackage{algorithmic}

% As of 2011, we use the hyperref package to produce hyperlinks in the
% resulting PDF.  If this breaks your system, please commend out the
% following usepackage line and replace \usepackage{icml2014} with
%\usepackage[nohyperref]{icml2014} above.
\usepackage{hyperref}

% Packages hyperref and algorithmic misbehave sometimes.  We can fix
% this with the following command.
\newcommand{\theHalgorithm}{\arabic{algorithm}}
\newcommand{\argmin}{\arg\!\min}

% Employ the following version of the ``usepackage'' statement for
% submitting the draft version of the paper for review.  This will set
% the note in the first column to ``Under review.  Do not distribute.''
\usepackage[accepted]{icml2014} 


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Objects Recognization Based on HOG with SVM}

\begin{document} 

\twocolumn[
\icmltitle{Objects Recognization Based on HOG with SVM\\Final Report for CIS 519}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2014
% package.
\icmlauthor{Shangyi Cheng}{shangyi@seas.upenn.edu}
\icmlauthor{Yao Chu}{chuyao@seas.upenn.edu}
\icmlauthor{Chenyang Zhao}{chzhao@seas.upenn.edu}

% You may provide any keywords that you 
% find helpful for describing your paper; these are used to populate 
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{machine learning, circle detection, feature extraction, objects recognization, neural network}

\vskip 0.3in
]

\begin{abstract} 
We studied the question of object recognization using histograms of oriented gradients (HOG) and support vector machine (SVM) with Gaussian Kernel on ball detection as a test case. After reviewing several existing methods for feature extraction, our project verified that HOG performs well in ball recognization. The whole process is shown in this report, from choosing regions of interest manually, extracting features from these regions using HOG, using principal component analysis (PCA) to reduce dimension of the  dataset and then applying SVM with Gaussian kernel to classify.   
\end{abstract}


\section{Introduction}
The topic for this project is to detect the object of our interest on the given images and predict which class it belongs to. The dataset we use is from Caltech 256(Griffin, G. Holub, AD. Perona, P.), which contain images of four kinds of balls, including 98 images of golf in the folder ``088.golf-ball'', 174 images of soccer ball in ``193.soccer-ball'', 104 images of bowling-ball in ``017.bowling-ball'' and 98 images of tennis ball in ``216.tennis-ball''. As shown in Figure \ref{fig:ROIResult}, among images of the same kind (take soccer-ball as example), some of the images have a whole contour of soccer in the center and fill the image, while in other images(such as a photo of a soccer game), the soccer may cover a small portion in the corner, or only part of a soccer or a group of soccers appear on the image. We decide to use the image with a single target occupying most of the image as the training set. So the first thing we need to do before training is to create such standard images for training from the raw dataset. And our final goal is, given a new image,  the model can detect whether this image contains any ball of our interest or just unrelated to balls and in the first condition, the model can give the label for the class the detected object belongs to.

\section{Methodology} 

\subsection{Overview of the Method}
Inspired by the paper \cite{dalal2005histograms} presented by Navneet Dalal and Bill Triggs, we tried the HOG method to extract the features vector for each individual image and apply SVM learner to train the dataset. An overview of the whole process is shown in Figure \ref{fig:overview}.\\
For the training process, we first preprocess the images by selecting the region containing the ball manually, resize and restore such images as the dataset for feature extraction. Then, HOG is applied on those processes images and get a vector of length $900$ as features for one instance. Thus, we reconstruct the training dataset by standardizing the dataset and apply PCA method to reduce the number of features. Then, SVM with Gaussian kernel is used to learn from the dataset. The metrics of accuracy, ROC and learning curve along with cross-validation are used to ensure that the model has learned to classify four kinds of balls with an acceptable accuracy without overfitting.\\
While detecting ball in an brand-new image, the model scan the image using mask with increasing size, doing the corresponding manipulation with the same parameters as those for training set, and then make prediction for each block no matter whether it contains any ball of our interest, what the portion of the ball covers the image and whether it appears in the center or in the corner, individually or in group. Among all predictions, the model will choose the ones with high confidence and decide whether they point to the same object. Finally, the target of our interest will be framed in a rectangle along with its label.

\begin{figure}[htp]
\centering
\includegraphics[width=0.5\textwidth]{overview.jpg}
\caption{An illustration of HOG with SVM process.}
\label{fig:overview}
\end{figure}

\subsection{Region of Interest (ROI) Selection}
Before we use HOG to extract features from labeled images, we need to select the ROI which contains an object we would like our model to recognize. We tried several ways to select such region, including Harris corner detection, Hough transfer, \cite{brown2005multi},\cite{cirDetect1}, \cite{cirDetect2}, \cite{cirDetect3}.\\
%At the beginning, we considered using corner detection directly on the original image to select ROI and at the same time extract features. However, as shown in Figure \ref{fig:crd}, extracting corner features based on the Harris Corner Detection and Adaptive Non-maximal Suppression method \cite{brown2005multi} is not so helpful for following reasons: firstly, different pattern can be found in a single kind of balls so that geometric corner feature doesn't provide enough information for classification; secondly, the complicated background may generate large amount of features which are irrelevant to the ball.\\
%\begin{figure}[htp]
%\centering
%\includegraphics[width=0.4\textwidth]{CornerDetection.jpg}
%\caption{Corner Detection Results}
%\label{fig:crd}
%\end{figure}
%Then we tried several methods for circle detection including \cite{cirDetect1}, \cite{cirDetect2}, \cite{cirDetect3} to pick up the circular region and build the training set. One of the robustest approach is Circle Hough Transform (CHT), which takes the image $M$ and desired radius range $[r_{min},r_{max}]$ as inputs and return the positions of circle centers. As shown in Figure \ref{fig:cird}, this method preforms good for the standard images with a single ball occupyting the whole image but has a poor performance when there are other ball-like distractions such as heads and the three holes on a bowling-ball, or only partial of the object, rather than a whole circle, appear on the image. It's also possible for CHT to return some position with high confidence for some circular patterns inside the ball. It's really hard to pick up the ROI just by the circular shape and without any referring to the patterns and features inside the region. \\
%\begin{figure}[htp]
%\centering
%\includegraphics[width=0.4\textwidth]{circleDetection.jpg}
%\caption{Unseccessful CHT Detection Results}
%\label{fig:cird}
%\end{figure}
%Thus, we finally decided to select ROI for each image in dataset manually. 
To increase the number of instances in our training data, for each image, we firstly decide the a rectangle ROI region and then shift the area a little in eight directions respectively. Therefore, a raw image will create at most nine images and at least one image for training. Finally, the picked area is resized to a $40 \times 40$ pixal square. Apart from the four kinds of balls, we also generate some squares with the label "Unknown" to represent the unrelated features to ball. The images in this class are mostly selected from the background, fragments of the ball and other unrelated area. Figure \ref{fig:ROIResult} shows the processed images after manual selection and resizing. First two rows are images for balls and the third row is for "Unknown".
\begin{figure}[htp]
\centering
\includegraphics[width=0.4\textwidth]{ROISelection.jpg}
\caption{Resized Images Before \& After ROI Selection}
\label{fig:ROIResult}
\end{figure}

\subsection{Histogram of Oriented Gradients Discriptors}
As described in ROI selection section, the patches cropped out were resized to 40x40 pixels. Each patch works as an dataset instance, however, there are two primary ways to extract features from these patches. First, each pixel value (grayscale, RGB or LAB color spaces) can be extracted directly as a feature. Second, apply HOG or SIFT to extract features. Considering the HOG/SIFT representation can capture edges and gradient structure which is characteristic of local shape as well as easily maintain the invariance to local geometric and photometric transformation, we finally chose second method. Also, HOG method can be implemented easily with fewer features compared to SIFT, so we chose to use HOG discriptors.\\  
Implementing HOG to extract features from patches followed procedure shown in figure \ref{fig:HOG}. For balls detection and classifier, following coefficient were chosen: All patches were converted into grayscale color space to normalize gamma and color; 1-D point derivative(centered [-1, 0, 1]) mask with no smoothing was applied to patches; 9 orientation bins evenly spaced over 0-180 degrees (unsigned gradient) was chosen; Cell size 6x6 and block size 2x2 were choosen for R-HOG.  
\begin{figure}[htp]
\centering
\includegraphics[width=0.4\textwidth]{HOG.jpg}
\caption{HOG Procedure}
\label{fig:HOG}
\end{figure}

HOG features is shown in figure \ref{fig:HOGtest}. After extract HOG features from a patch, reshape the feature matrix into a vector represent the patch. Eventually we generated a 3660x900 datasets with 3660 instances and 900 features. 

\begin{figure}[htp]
\centering
\includegraphics[width=0.4\textwidth]{hogtest.jpg}
\caption{HOG Feature Extraction Result}
\label{fig:HOGtest}
\end{figure}

\subsection{PCA}
The number of features we get using HOG is $900$. Since the number of instances we use is about $4000$, which is not large enough for $900$ features and easy to result in overfit, it's necessary to reduce the dimension first. On the other hand, applying PCA would improve efficiency and decrease the time for training. On the training dataset, we firstly standardize the dataset $X$ and record covariance matrix $\Sigma$ and mean values $\mu$. Next, we choose the $d$ most important PCA basis vectors by calculating the eigenvalues and eigenvectors of $X^TX$. Then we reconstruct the new training dataset by multiply the PCA basis vectors to the origin dataset $X$. %While predicting class for a new instance $\mathbf{x_new}$, Then $\Sigma$ and $X$ got previously on training dataset should be used on the test data.
\subsection{One-Class SVM}
Support vector machine(SVM) is one of basic learning algorithm that most widely used to solve object recognition problems \cite{dalal2005histograms},\cite{SVMforRecogintion}. In our case, we applied one-class SVM first and extend it to multi-class to train the dataset with several different labels.\\
Given a set of instances which belongs to either of two classes, a SVM classifier finds the optimal hyperplane leaving the largest possible fraction of instances of the same class on the same side, while maximize the distance of either class from hyperplane. The objective function of SV learner is
\begin{equation}
\argmin_\theta \frac{1}{2}\sum_{i=1}^d \theta_i^2
\end{equation}
s.t. $y_j(\mathbf{\theta x_j})\geq 1$ $\forall j$.\\
The problem of minimizing the cost function could be simplified as maximizing the $J(\alpha)$ in \ref{eq:Jfunc}.
\begin{equation}
J(\alpha) = \sum_{i=1}^n \alpha_i - \frac{1}{2}\sum_{i=1}^n\sum_{j=1}^n \alpha_i \alpha_jy_iy_j\langle x_i,x_j\rangle
\label{eq:Jfunc}
\end{equation}
s.t. $\alpha_i\leq 0 \forall i$, $\sum_i \alpha_iy_i = 0$,
where $\alpha_i$ is the constraints weight scaler, and $\langle x_i,x_j \rangle$ is a scaler given by the kernel function.\\
Take linear kernel as an example, the parameter of the hyperplane follows that
\begin{equation}
\mathbf{\theta} = \sum_{i=1}^N\alpha_i y_i x_i
\end{equation}
while $b$ can be determined from $\alpha$ solution of the dual problem, and from the Kühn-Tucker conditions
\begin{equation}
\alpha_i(y_i(\theta x_i+b)-1) = 0, i = 1,2,...,N
\end{equation}
The problem of classifying a new data instance could be simply solved by looking at the sign of the score function 
\begin{equation}
P(\mathbf{x}) = \mathbf{\theta}\mathbf{x}+\mathbf{b}
\label{eq:score}
\end{equation}
While applying SVM algorithm, choosing proper kernel is essential to train the classifier. We used two different kernels to train the dataset, which are Gaussian kernel, linear kernel, and drew the Receiver	Operating Characteristic(ROC) curve respectively. The result are shown in the figure (\ref{fig:ROC1},\ref{fig:ROC2}). As shown in the figure, the two kernels give similar result while Gaussian kernel provides a faster speed to converge.\\
\begin{figure}[htb]
\centering
\includegraphics[width = 0.4\textwidth]{ROCgauss.jpg}
\caption{ROC curve of SVM with Gaussian Kernel}
\label{fig:ROC1}
\end{figure}

\begin{figure}[htb]
\centering
\includegraphics[width = 0.4\textwidth]{ROClinear.jpg}
\caption{ROC curve of SVM with Linear Kernel}
\label{fig:ROC2}
\end{figure}
\subsection{Multi-Class SVM}
Traditional SVM algorithm only solves 2-classes problems. However, detecting ball object in a image should be able to tell whether there is a ball and figure out the type of the ball object, which is a multi-class classification problem. Therefore, one-vs-rest is introduced for solving multi-class problem.\\
Given a dataset with $k$ different labels, $k$ different one-class SVM classifiers is trained for each class. When training for individual class, the instances that belongs to this class are labeled as $1$, and all the others are labeled as $0$. A new instance should be predicted as in the class which gives the largest value from score function \ref{eq:score}.\\
\subsection{Detect Balls from New Image}
Once classifier was trained, balls could be detected from the new input image based on the predict results. \\
In order to detect the balls, the algorithm should search every possible detect window in the image. So square detect window with changeable size move around in the image to extract test patches. Window size changes from minimum value (80 pixels or 10 percent of maximum side) to maximum value ( small one between image width and height). And overlapping ratio between two consecutive detect windows is 85 percent.  Every patch is resized to 40x40 pixels, extracted HOG features, and normalized in the same way with training data. Following this, apply classifier to predict labels for test instances. Based on the predict results, overlap test instances can be eliminated by adaptive non-maximum suppression. A neighbour positive detection threshold is set to improve the detection result. 

\section{Results and Performance Study}
\subsection{Learning Curve}
To evaluate our SVM classifier, we run our classifier through 10-fold cross validation and drew the learning curve in figure (\ref{fig:learning curve}) to show the performance versus the number of training examples. As shown in the figure, the testing accuracy increases as the set of training instances grows larger, and the variance of accuracies decreases. 
\begin{figure}[htb]
\centering
\includegraphics[width = 0.4\textwidth]{learningcurve.jpg}
\caption{Learning Curve of testing accuracy}
\label{fig:learning curve}
\end{figure}

\subsection{Results on Raw Images}
Using the model after learning, we test on several raw images, including the ones where we selected ROI for training and new images which the model never see before. In Figure \ref{fig:succefulResult}, we show some of the results that our model detect and label the target successfully. We use different colors of rectangle to frame the area of the detected target, red for soccer, cyan for tennis, purple for bowling-ball and green for golf. In theory, our model can detect all balls with the same label on an image, such as the upper middle and right ones with several tennis balls. The lower middle image with golf is a brand-new image from website and our model can detect and label it correctly. \\

\begin{figure}[htb]
\centering
\includegraphics[width=0.5\textwidth]{successfulResult.jpg}
\caption{Successful Results for Detection and Label}
\label{fig:succefulResult}
\end{figure}

\begin{figure}[htp]
\centering
\includegraphics[width=0.5\textwidth]{failedResult.jpg}
\caption{False Positive and True Negative Results}
\label{fig:failedResult}
\end{figure}

However, there are also some cases in which our model either fails to detect all the targets (as the right one with several bowling-balls in Figure \ref{fig:failedResult}), or gives false positive prediction (label some irrelevant areas as a specific kind of ball, such as the left and middle ones). One of the possible reasons is that the target is sheltered by other objects have regular patterns, such as the soccer in the net shown Figure \ref{fig:failedResult}. Another reason is related to the size of the raw images. The image with a lifting hand holding a tennis has $96\times96$ pixels. Since the background of it is also green, it's more likely to detect and label the background mistakenly as tennis.


\section{Further Work} 
Although our current Gaussian SVM detector is reasonably accurate for the cropped patches (test part of manually cropped ROI), there is still a problem when test thousands of test instances extracted from new input image (downloaded from other sources). False positive results were the main problem we encountered. The most likely explanation for this problem we believe is the insufficiency of the datasets, especially in the negative sets ('Unknown'). Also the original datasets for each kind ball is limited, the cropped patches generated based on the original datasets may be not representative. It would be useful to use a larger datasets to improve detector accuracy and implement cascade training method \cite{viola2001rapid} to improve the training efficiency.

%The algorithm is presented in Algorithm \ref{alg:Patch}.
%\begin{algorithm}[tb]
%   \caption{Select ROI Manually}
%   \label{alg:Patch}
%\begin{algorithmic}
%   \STATE {\bfseries Input:} image $I$
%   \STATE Initialize accumulator matrix $M = 0$.
%   \FOR{each edge point$(i,j)$ in image $I$}
%   \FOR{each point $(m,n)$ in image $I$}
%   \IF{$r_{min}^2<(m-i)^2+(n-j)^2<r_{max}^2$}
%   \STATE {$M(m,n) = M(m,n)+\phi_{m,n}$}
%   \ENDIF
%   \ENDFOR
%   \ENDFOR
%   \STATE {\bfseries Return:} positions of local maximum points in accumulator matrix $M$
%\end{algorithmic}
%\end{algorithm}\\

\section*{Acknowledgments} 
This project is supported by Uenn CIS 519. Thanks Eric Eaton and Xiaoxiang Hu for their instruction. Thanks MATLAB for providing so many useful toolboxes.

\bibliography{report-ChengChuZhao}
\bibliographystyle{icml2014}

\end{document} 


